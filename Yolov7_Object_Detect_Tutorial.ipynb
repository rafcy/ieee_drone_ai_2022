{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.kios.ucy.ac.cy/evai/wp-content/uploads/2022/10/MicrosoftTeams-image-8-e1665993611643.png)  ![](https://www.kios.ucy.ac.cy/evai/wp-content/uploads/2022/10/MicrosoftTeams-image-9-e1665993618291.png)"
      ],
      "metadata": {
        "id": "Ihs5uJYg8av0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv7 - Object Detection Pipeline Tutorial\n"
      ],
      "metadata": {
        "id": "RkzqWqAZeucw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Yolov7 Object Detection Pipeline Tutorial***\n",
        "\n",
        "***IEEE Drone A.I. School 2022***\n",
        "\n",
        "***Author: Rafael Makrigiorgis***\n",
        "\n",
        "***Date: October 2022***\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This tutorial is based on the [YOLOv7 repository](https://github.com/WongKinYiu/yolov7) by WongKinYiu. This notebook shows training on your own custom objects. Many thanks to WongKinYiu and AlexeyAB for putting this repository together.\n",
        "\n",
        "### **Steps Covered in this Tutorial**\n",
        "To train our detector we take the following steps:\n",
        "\n",
        "1.  Learn about YOLO\n",
        "3.  Download and Install YOLOv7 dependencies\n",
        "4.  Prepare the custom dataset\n",
        "5.  Run YOLOv7 training\n",
        "6.  Evaluate YOLOv7 performance\n",
        "7.  Run YOLOv7 inference on test images / sample video"
      ],
      "metadata": {
        "id": "WvjC0VAaKG1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YOLO"
      ],
      "metadata": {
        "id": "XJt2EQIwPeSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is YOLO?"
      ],
      "metadata": {
        "id": "c1G1L2AFkeBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLO** is an abbreviation for the term **‘You Only Look Once’**. This is an algorithm that **localizes** and **classifies** various **objects in a picture** (in real-time). \n",
        "\n",
        "The original YOLO model was introduced in the paper “You Only Look Once: Unified, Real-Time Object Detection” in 2015[1]. At the time, RCNN models[2,3,4] were the best way to perform object detection, and their time consuming, multi-step training process made them cumbersome to use in practice. \n",
        "YOLO was created to do away with as much of that hassle as possible, by offering single-stage object detection they reduced training & inference times as well as massively reduced the cost to run object detection.\n",
        "As the name suggests, the algorithm requires only a single forward propagation through a neural network to detect objects.\n",
        "![](https://opencv-tutorial.readthedocs.io/en/latest/_images/yolo1_net.png)\n",
        "\n",
        "[Image Source](https://arxiv.org/pdf/1506.02640.pdf)\n",
        "\n",
        "YOLO algorithm aims to predict a class of an object and the bounding box that defines the object location on the **input image**. It recognizes objects and **outputs their bounding boxes** using four numbers:\n",
        "\n",
        "*   Center of the bounding box - *Bx, By*\n",
        "*   Width of the box - *W*\n",
        "*   Height of the box - *H*\n",
        "\n",
        "In addition to that, YOLO predicts the corresponding number c for the predicted **class** as well as the **probability** of the prediction *P(c)*\n",
        "\n",
        "In order to be able to get these output parameters, we extract a **weights** file which contains the final numbers for each of the deep learning filters. It can be used for either transfer learning, set as initial weights for a new training set, or to detect the objects it is trained on."
      ],
      "metadata": {
        "id": "DlSFWEzzeqiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why the YOLO algorithm is important"
      ],
      "metadata": {
        "id": "K2rpionfkgda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO algorithm is important because of the following reasons:\n",
        "\n",
        "\n",
        "*   **Speed**: This algorithm improves the speed of detection because it can predict objects in real-time.\n",
        "\n",
        "*   **High accuracy**: YOLO is a predictive technique that provides accurate results with minimal background errors.\n",
        "\n",
        "*   **Learning capabilities**: The algorithm has excellent learning capabilities that enable it to learn the representations of objects and apply them in object detection.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8mZMWLDrJJTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does YOLO works?"
      ],
      "metadata": {
        "id": "qsGXPtplkmOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO works to perform object detection in a single stage by first separating the image into N grids. Each of these grids is of equal size SxS. Each of these regions is used to detect and localize any objects they may contain. For each grid, bounding box coordinates, B, for the potential object(s) are predicted with an object label and a probability score for the predicted object's presence.\n",
        "\n",
        "As you may have guessed, this leads to a significant overlap of predicted objects from the cumulative predictions of the grids. To handle this redundancy and reduce the predicted objects down to those of interest, YOLO uses [Non-Maximum Suppression](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c) to suppress all the bounding boxes with comparatively lower probability scores.\n",
        "\n",
        "To achieve this, YOLO first compares the probability scores associated with each decision, and takes the largest score. Following this, it removes the bounding boxes with the largest Intersection over Union with the chosen high probability bounding box. This step is then repeated until only the desired final bounding boxes remain.\n",
        "\n",
        "![](https://i.ibb.co/VCCfGTZ/grid-ssd.jpg)\n",
        "\n",
        "![](https://i.ibb.co/qRY29DX/yolossd.png)\n",
        "\n",
        "(a)Image Divided into Grids   (b) Before Non- Maximum Suppression  (c)After Non Maximal Suppression (Final Output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0wi7XcE4klhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Install YOLOv7 \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1VoV5iG9m5nf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the repository of YOLOv7"
      ],
      "metadata": {
        "id": "HFsR7w3VnFc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk4o8NRh5p8W"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/WongKinYiu/yolov7.git "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change directory to the cloned folder."
      ],
      "metadata": {
        "id": "xVDU6IMOnBJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov7"
      ],
      "metadata": {
        "id": "gL6dmVca6-Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies\n",
        "*Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU*\n",
        "\n"
      ],
      "metadata": {
        "id": "Fgs5EMg5nJ-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "-6tLAaLt7DXk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Custom Dataset\n"
      ],
      "metadata": {
        "id": "KXl8mvE39gtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotation - Labelimg"
      ],
      "metadata": {
        "id": "7eDZcufk9lbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download locally on your computer the sample images from [here](https://www.kios.ucy.ac.cy/evai/wp-content/uploads/2022/10/labeling_sample.zip).\n",
        "\n",
        "It contains aerial images of vehicles that needs to be labeled using the Labelimg tool mentioned below."
      ],
      "metadata": {
        "id": "pMaZyxE0OzAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LabelImg is a graphical image annotation tool.\n",
        "\n",
        "It is written in Python and uses Qt for its graphical interface.\n",
        "\n",
        "Annotations are saved as XML files in PASCAL VOC format, the format used by ImageNet. Besides, it also supports YOLO and CreateML formats.\n",
        "\n",
        "![](https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo3.jpg)\n"
      ],
      "metadata": {
        "id": "JR4POnyS-rPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation\n",
        "\n",
        "\n",
        "*   Follow the instructions from the [github](https://github.com/heartexlabs/labelImg#installation) or,\n",
        "*   Download  [this](https://github.com/tzutalin/labelImg/files/2638199/windows_v1.8.1.zip) ( for windows only)\n",
        "\n"
      ],
      "metadata": {
        "id": "r6hMkr9cGqaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions\n",
        "1. Create pre-defined classes. You can edit the data/predefined_classes.txt to load pre-defined classes\n",
        "2.  Copy the existing lables file to same folder with the images. The labels file name must be same with image file name.\n",
        "3.  Click File and choose 'Open Dir' then Open the image folder.\n",
        "4.  Select image in File List, it will appear the bounding box and label for all objects in that image.\n",
        "(Choose Display Labels mode in View to show/hide lablels)\n",
        "5.  We only need YOLO format\n",
        "\n",
        "**The output of the labelimg for each image is a txt file that contains the bounding boxes you created with the following format \"class_id center_x center_Y width height\"**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OVA0Ps0K9-sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hotkeys\n",
        "*  Ctrl + u\tLoad all of the images from a directory\n",
        "*  Ctrl + r\tChange the default annotation target dir\n",
        "*  Ctrl + s\tSave\n",
        "*  Ctrl + d\tCopy the current label and rect box\n",
        "*  Ctrl + Shift + d\tDelete the current image\n",
        "*  Space\tFlag the current image as verified\n",
        "*  w\tCreate a rect box\n",
        "*  d\tNext image\n",
        "*  a\tPrevious image\n",
        "*  del\tDelete the selected rect box\n",
        "*  Ctrl++\tZoom in\n",
        "*  Ctrl--\tZoom out\n",
        "*  ↑→↓←\tKeyboard arrows to move selected rect box"
      ],
      "metadata": {
        "id": "Gl1_du8lIL2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare custom data"
      ],
      "metadata": {
        "id": "0MfVL0bVl8AZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon Labeling your custom dataset do the following you should split your custom dataset into train, validation and test samples. Usually we split them into 80% of your data for training and 20% for validation and testing.\n",
        "\n"
      ],
      "metadata": {
        "id": "rcU__sm6XqVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a folder for your training data"
      ],
      "metadata": {
        "id": "ATgDNHbycy-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir vehicles"
      ],
      "metadata": {
        "id": "JRowO0FlmTf-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change directory to the new folder, then download and unzip the custom dataset."
      ],
      "metadata": {
        "id": "QirzJ87pmESI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd vehicles\n",
        "!wget https://www.kios.ucy.ac.cy/evai/wp-content/uploads/2022/10/DJI-405-720p.zip"
      ],
      "metadata": {
        "id": "71yTjeBQmDLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"*****.zip\""
      ],
      "metadata": {
        "id": "BAbIP5IHd2x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split your custom data to train/test/valid text files which contains the filepaths for each image."
      ],
      "metadata": {
        "id": "OIoCxaP5d9IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "directory = os.getcwd() + \"****/folderpath\"\n",
        "# Percentage of images to be used for the test set\n",
        "percentage_test = 20\n",
        "\n",
        "# Create and/or truncate train.txt and test.txt\n",
        "file_train = open('train.txt', 'w')\n",
        "file_valid = open('valid.txt', 'w')\n",
        "file_test = open('test.txt', 'w')\n",
        "\n",
        "# Populate train.txt and test.txt\n",
        "counter = 1\n",
        "index_test = round(100 / percentage_test)\n",
        "filenames = []\n",
        "test_c = 0\n",
        "for path, subdirs, files in os.walk(directory):\n",
        "\n",
        "    for filename in files:\n",
        "        infilename = os.path.join(path, filename)\n",
        "        if not os.path.isfile(infilename): continue\n",
        "        if infilename.endswith('.jpg') or infilename.endswith('.JPG') or infilename.endswith('.PNG') or infilename.endswith('.png'):  # check if ifle is txt format\n",
        "            filenames.append(path+'/'+filename)\n",
        "        random.shuffle(filenames)\n",
        "for filename in filenames:\n",
        "    if counter == index_test:\n",
        "        counter = 1\n",
        "        if test_c == 0:\n",
        "          file_test.write(filename + \"\\n\")\n",
        "          test_c = 1\n",
        "        else:\n",
        "          file_valid.write(filename + \"\\n\")\n",
        "          test_c = 0\n",
        "    else:\n",
        "        file_train.write(filename  + \"\\n\")\n",
        "        counter = counter + 1\n",
        "    index_test = round(100 / percentage_test)"
      ],
      "metadata": {
        "id": "f3r34NA-aTpR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Navigate to yolov7 folder.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qu3ODgkAo09n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov7"
      ],
      "metadata": {
        "id": "0MepxmtpqMug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare training files"
      ],
      "metadata": {
        "id": "N6S5lbb4dO8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be needing the following files:\n",
        "*   yolov7.yaml => it contains the model filters of the neural network. It can be found in \"cfg/training\" folder\n",
        "*  vehicles.yaml => contains information about our data, modify the one that it can be found in \"data/coco.yaml\"\n",
        "*  yolov7_training.pt => pre-trained weights of yolov7 trained on [COCO dataset](https://cocodataset.org/) "
      ],
      "metadata": {
        "id": "kMgbNjE3p3EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Copy the model yaml file into our training folder."
      ],
      "metadata": {
        "id": "B-8CKeRpsGzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp cfg/training/yolov7.yaml vehicles/yolov7.yaml"
      ],
      "metadata": {
        "id": "2TtKJmAcsWwQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Modify yolov7-tiny.yaml 'nc' parameter with the number of your classes.\n"
      ],
      "metadata": {
        "id": "WKHByiZxspNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Download yolov7 per-trained model to be used as initial weights during training"
      ],
      "metadata": {
        "id": "wHfbvi_dthIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt"
      ],
      "metadata": {
        "id": "834tdaWNqI0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Copy the data yaml file into our training folder.\n"
      ],
      "metadata": {
        "id": "mMq4Vv5LuEQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp data/coco.yaml vehicles/vehicles_data.yaml"
      ],
      "metadata": {
        "id": "odBfB_mftpIa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Edit your file by adding the content that corresponds to your custom dataset\n",
        "\n",
        "This file should contain the paths for the text files which contains the images path for each corresponding data set that will be used for training, validation and testing. An example is depicted below.\n",
        "\n",
        "\n",
        "```\n",
        "# train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]\n",
        "train: data/crowd_human/train.txt  # train images\n",
        "val: data/crowd_human/valid.txt    # valid images\n",
        "test: data/crowd_human/test.txt    # test images\n",
        "\n",
        "# number of classes\n",
        "nc: 2\n",
        "\n",
        "# class names\n",
        "names: [ 'person', 'head']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "yF5Zycg4uSaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training your custom dataset\n"
      ],
      "metadata": {
        "id": "7k10DjEidrMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --workers 16 --device 0 --epochs 30 --batch-size 16 --data \"data_folder/data.yaml\" --img 512 512 --cfg \"data_folder/yolov7.yaml\" --weights \"yolov7_training.pt\" --name yolov7-custom --hyp data/hyp.scratch.custom.yaml\n"
      ],
      "metadata": {
        "id": "-BwBiXBEdwqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating your trained model"
      ],
      "metadata": {
        "id": "TPGhnmY6ei0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --data \"data/data.yaml\" --img 512 --batch 16 --iou 0.65 --device 0 --weights \"runs/train/****/weights/best.pt\" --name yolov7_640_val"
      ],
      "metadata": {
        "id": "c2CWh8f-ekd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the output of the script by going to \"runs/test/exp#\"\n"
      ],
      "metadata": {
        "id": "yEIGnCrK7S1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/5_precision%20and%20recall.png)\n",
        "\n",
        "![](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/6_precision%20and%20recall.png) ~[](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/6_precision%20and%20recall.png)\n",
        "\n",
        "**Average Precision(AP):**\n",
        "The general definition for the Average Precision (AP) is finding the area under the precision-recall curve above.\n",
        "\n",
        "**mAP (mean average precision):**\n",
        "Is the average of AP. In some context, we compute the AP for each class and average them."
      ],
      "metadata": {
        "id": "4p0-9OsXW26O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract detections"
      ],
      "metadata": {
        "id": "yoTJTxMV4-9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the sample video and run the detection script on it."
      ],
      "metadata": {
        "id": "8utU1yNMehdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.kios.ucy.ac.cy/evai/wp-content/uploads/2022/10/DJI_0406_cut.mp4"
      ],
      "metadata": {
        "id": "kG2G6-9megdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights \"runs/train/*****/weights/best.pt\" --conf 0.25 --img-size 512 --source \"videofile.mp4\" --device 0"
      ],
      "metadata": {
        "id": "HcttPMGF4nG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the output video in folder 'runs/detect/exp#'."
      ],
      "metadata": {
        "id": "JerIg9n05FTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also run the detection on a whole folder with images.\n",
        "\n",
        "Download and unzip the following images sample."
      ],
      "metadata": {
        "id": "Sa0YhU7re7TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.kios.ucy.ac.cy/evai/wp-content/uploads/2022/10/mtihani.zip"
      ],
      "metadata": {
        "id": "AcvQ41VcfFhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"filename.zip\""
      ],
      "metadata": {
        "id": "cFpVk7MJfOSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights \"runs/train/******/weights/best.pt\" --conf 0.25 --img-size 512 --source \"image_folder/\" --device 0"
      ],
      "metadata": {
        "id": "Vy6FoeZK5D3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the output images in folder 'runs/detect/exp#'."
      ],
      "metadata": {
        "id": "DSmKAV0RZKXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "7FXRwixgy8E1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   *Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.*\n",
        "2.   *Girshick, Ross, et al. \"Rich feature hierarchies for accurate object detection and semantic segmentation.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.*\n",
        "3.   *Girshick, Ross. \"Fast r-cnn.\" Proceedings of the IEEE international conference on computer vision. 2015.*\n",
        "4.   *Ren, Shaoqing, et al. \"Faster r-cnn: Towards real-time object detection with region proposal networks.\" Advances in neural information processing systems 28 (2015).*\n",
        "\n"
      ],
      "metadata": {
        "id": "H2Y1E2EhyV81"
      }
    }
  ]
}